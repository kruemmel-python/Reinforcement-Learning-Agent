# ki_agent_konfigurierbar.py
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import collections
from sklearn.metrics.pairwise import cosine_similarity # Import f√ºr Kosinus-√Ñhnlichkeit
import yaml # Import f√ºr YAML

# üß† Ged√§chtnis: Speichert Erlebnisse & Erkenntnisse
class Memory:
    def __init__(self, capacity=1000, causal_capacity=100): # Kapazit√§t f√ºr kausales Ged√§chtnis hinzugef√ºgt
        self.capacity = capacity
        self.memory = collections.deque(maxlen=capacity)
        self.causal_memory = collections.deque(maxlen=causal_capacity) # Separates kausales Ged√§chtnis mit Capacity

    def store(self, experience):
        """Speichert eine Erinnerung (Erlebnis, Emotion, Reaktion)."""
        self.memory.append(experience)

    def recall(self):
        """Ruft zuf√§llige Erinnerung ab."""
        if len(self.memory) > 0:
            return random.choice(self.memory)
        return None

    def store_causal(self, state, action, next_state, reward): # Reward hinzugef√ºgt
        """Speichert eine kausale Erinnerung (Zustand, Aktion, n√§chster Zustand, Belohnung)."""
        self.causal_memory.append((state, action, next_state, reward)) # Reward gespeichert

    def recall_causal(self):
        """Ruft zuf√§llige kausale Erinnerung ab."""
        if len(self.causal_memory) > 0:
            return random.choice(self.causal_memory)
        return None

    def recall_similar(self, state, tolerance=0.5):
        """Ruft √§hnliche Erinnerung ab, basierend auf Zustands√§hnlichkeit."""
        similar_memories = []
        for memory in self.memory:
            if memory and torch.allclose(torch.tensor(memory[0]), state.squeeze(0), atol=tolerance):
                similar_memories.append(memory)
        if similar_memories:
            return random.choice(similar_memories)
        return None


# üåê Weltmodell (Graph-basiert - rudiment√§r)
class WorldModel:
    def __init__(self, state_size=3, action_size=1, prediction_model_hidden_size=16, learning_rate=0.001): # Prediction Model Parameter, action_size korrigiert zu 1
        self.graph = {}  # Graph als Dictionary: Zustand -> {Aktion: [(n√§chster_Zustand, Belohnung, Zustandseigenschaften)]}
        self.state_visit_counts = collections.defaultdict(int) # Z√§hler f√ºr Zustandbesuche (vorerst beibehalten)
        self.state_reward_history = collections.defaultdict(collections.deque) # Reward-History pro Zustand (f√ºr Valence - vorerst beibehalten)
        self.reward_history_window = 10 # Fenstergr√∂√üe f√ºr Reward-History

        # **Neu: Vorhersagemodell (Prediction Model)**
        self.prediction_model = nn.Sequential(
            nn.Linear(state_size + action_size, prediction_model_hidden_size), # Zustand + Aktion als Input
            nn.ReLU(),
            nn.Linear(prediction_model_hidden_size, state_size + 1) # Output: n√§chster Zustand + Belohnung (1 Wert f√ºr Belohnung)
        ).float() # Stelle sicher, dass das Modell Float verwendet
        self.prediction_model_optimizer = optim.Adam(self.prediction_model.parameters(), lr=learning_rate)
        self.prediction_model_loss_fn = nn.MSELoss() # MSE Loss f√ºr Zustand und Belohnung


    def add_transition(self, state, action, next_state, reward):
        """F√ºgt eine Transition zum Weltmodell-Graphen hinzu, trainiert Vorhersagemodell & speichert Zustandseigenschaften."""
        state_tuple = tuple(state.tolist()) # Zustand als Tuple f√ºr Dictionary-Key
        next_state_tuple = tuple(next_state.tolist())

        # Zustandbesuchs-Z√§hler aktualisieren (vorerst beibehalten)
        self.state_visit_counts[state_tuple] += 1

        # Reward-History aktualisieren (vorerst beibehalten)
        self.state_reward_history[state_tuple].append(reward)
        if len(self.state_reward_history[state_tuple]) > self.reward_history_window:
            self.state_reward_history[state_tuple].popleft() # √Ñlteste Belohnung entfernen

        # **Neu: Vorhersagemodell trainieren**
        self.train_prediction_model(state, action, next_state, reward)

        # **Neu: Zustandseigenschaften (ML-basiert) - werden DYNAMISCH in calculate_state_properties berechnet**
        state_properties = self.calculate_state_properties(state, action) # Berechne Eigenschaften (jetzt dynamisch)
        next_state_properties = self.calculate_state_properties(next_state, action) # Eigenschaften f√ºr n√§chsten Zustand (auch dynamisch)


        if state_tuple not in self.graph:
            self.graph[state_tuple] = {}
        if action not in self.graph[state_tuple]:
            self.graph[state_tuple][action] = []
        self.graph[state_tuple][action].append((next_state_tuple, reward, next_state_properties)) # Speichere Eigenschaften


    def train_prediction_model(self, state, action, next_state, reward):
        """Trainiert das Vorhersagemodell mit einer einzelnen Transition."""
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0) # Batch-Dimension
        action_tensor = torch.tensor([action], dtype=torch.float32).unsqueeze(0) # Batch-Dimension
        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0) # Batch-Dimension
        reward_tensor = torch.tensor([reward], dtype=torch.float32).unsqueeze(0) # Batch-Dimension

        # Kombiniere Zustand und Aktion zum Input des Modells
        model_input = torch.cat([state_tensor, action_tensor], dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren

        # Zielwerte: N√§chster Zustand und Belohnung
        target_output = torch.cat([next_state_tensor, reward_tensor], dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren

        # Vorhersage und Loss berechnen
        predicted_output = self.prediction_model(model_input)
        loss = self.prediction_model_loss_fn(predicted_output, target_output)

        # Backpropagation und Optimierung
        self.prediction_model_optimizer.zero_grad()
        loss.backward()
        self.prediction_model_optimizer.step()


    def get_transitions(self, state):
        """Ruft Transitionen f√ºr einen gegebenen Zustand ab."""
        state_tuple = tuple(state.tolist())
        return self.graph.get(state_tuple, {})

    def recall_similar_state(self, current_state, tolerance=0.5):
        """
        Ruft einen √§hnlichen Zustand aus dem Weltmodell ab, basierend auf Zustands√§hnlichkeit.
        Gibt den √§hnlichen Zustand (als Tensor) und seine Eigenschaften oder None zur√ºck.
        """
        current_state_tensor = torch.tensor(current_state)
        for state_tuple in self.graph:
            state_tensor = torch.tensor(state_tuple)
            if torch.allclose(state_tensor, current_state_tensor, atol=tolerance):
                # **Neu: Zustandseigenschaften zur√ºckgeben**
                transitions = self.graph.get(state_tuple, {})
                if transitions: # Transitionen vorhanden
                    _, _, properties = transitions[list(transitions.keys())[0]][0] # Erste Transition, erste Aktion, Zustandseigenschaften
                    return state_tensor.unsqueeze(0), properties # R√ºckgabe als Tensor mit Batch-Dimension und Eigenschaften
                return state_tensor.unsqueeze(0), {} # Zustand gefunden, aber keine Eigenschaften (sollte nicht passieren, aber sicherheitshalber)
        return None, {} # Kein √§hnlicher Zustand gefunden, leere Eigenschaften


    def calculate_state_properties(self, state, action):
        """
        Berechnet Zustandseigenschaften (ML-basiert).
        Aktuell: novelty_score, frustration_level, predictability (ML-basiert), valence (ML-basiert).
        """
        state_tuple = tuple(state.tolist())
        properties = {}
        properties['novelty_score'] = 1.0 # Neuigkeitswert (vereinfacht: immer 1.0 f√ºr neue Zust√§nde - vorerst beibehalten)
        properties['frustration_level'] = 0.0 # Frustration (vorerst statisch/heuristisch)

        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0) # Batch-Dimension
        action_tensor = torch.tensor([action], dtype=torch.float32).unsqueeze(0) # Batch-Dimension
        model_input = torch.cat([state_tensor, action_tensor], dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren

        # **Neu: Predictability (ML-basiert - Vorhersagefehler)**
        with torch.no_grad(): # Keine Gradientenberechnung f√ºr Zustandseigenschaften-Berechnung
            predicted_output = self.prediction_model(model_input)
            predicted_next_state = predicted_output[:, :state_tensor.shape[1]] # Vorhergesagter Zustand
            # Annahme: Tats√§chlicher n√§chster Zustand ist unbekannt zum Zeitpunkt der Berechnung der Eigenschaften des *aktuellen* Zustands.
            # Daher Approximation:  Wir nutzen den *vorhergesagten* n√§chsten Zustand als Referenz f√ºr den aktuellen Zustand.
            # **WICHTIG:** Dies ist eine Vereinfachung und k√∂nnte in Zukunft verfeinert werden.
            state_prediction_error = self.prediction_model_loss_fn(predicted_next_state, state_tensor) # MSE-Loss zwischen vorhergesagtem Zustand und aktuellem Zustand (als Approximation des Fehlers)
            properties['predictability'] = 1.0 / (state_prediction_error.item() + 1e-6) # Inverser Fehler (kleiner Fehler -> hohe Predictability) + kleine Konstante f√ºr Stabilit√§t


        # **Neu: Valence (ML-basiert - Vorhergesagte Belohnung)**
        with torch.no_grad(): # Keine Gradientenberechnung
            predicted_output = self.prediction_model(model_input)
            predicted_reward = predicted_output[:, state_tensor.shape[1]:] # Vorhergesagte Belohnung
            properties['valence'] = predicted_reward.item() # Vorhergesagte Belohnung als Valenz


        return properties



# ü§ñ Aufmerksamkeitsmechanismus (Self-Attention)
class AttentionMechanism(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).float() # batch_first=True hinzugef√ºgt, Float-Typ erzwingen

    def forward(self, query, key=None, value=None):
        if key is None:
            key = query.float() # Stelle sicher, dass key Float ist
        if value is None:
            value = query.float() # Stelle sicher, dass value Float ist
        query = query.float() # Stelle sicher, dass query Float ist
        attention_output, _ = self.attention(query, key, value)
        return attention_output


# üß† Dynamische Hierarchische Selbst-Attention
class DynamicHierarchicalSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, initial_layers=1, layer_growth_threshold=0.9): # Start mit einer Ebene, Schwellwert f√ºr Ebenenwachstum
        super().__init__()
        self.attention_layers = nn.ModuleList([AttentionMechanism(embed_dim, num_heads) for _ in range(initial_layers)])
        self.level_projections = nn.ModuleList([nn.Linear(embed_dim * 2, embed_dim).float() for _ in range(initial_layers-1)]) # Projektionen zwischen den Ebenen, Float-Typ
        self.inter_level_attention = nn.ModuleList([AttentionMechanism(embed_dim, num_heads) for _ in range(initial_layers-1)]) # Attention-Mechanismen ZWISCHEN den Ebenen
        self.layer_growth_threshold = layer_growth_threshold # Schwellwert f√ºr das Hinzuf√ºgen neuer Ebenen
        self.num_layers = initial_layers
        self.relu = nn.ReLU() # ReLU Aktivierungsfunktion als Klassenattribut
        self.embed_dim = embed_dim # Speichere embed_dim

    def forward(self, abstract_state):
        """Dynamische Hierarchische Selbst-Attention mit adaptiver Ebenenanzahl."""
        level_outputs = []
        current_level_input = abstract_state.float() # Stelle sicher, dass Input Float ist

        for i in range(self.num_layers): # Dynamische Anzahl Ebenen
            attention_layer = self.attention_layers[i] # Ebene dynamisch ausw√§hlen
            attention_output = attention_layer(current_level_input)
            level_outputs.append(attention_output)

            if i < self.num_layers - 1: # F√ºr Ebenen UNTERHALB der obersten Ebene:
                # Gerichtete Attention zwischen den Ebenen (wie zuvor)
                query_next_level = attention_output
                key_value_current_level = level_outputs[0] # Beispiel: Output der ERSTEN Ebene als Key/Value
                inter_level_output = self.inter_level_attention[i](query_next_level, key_value_current_level, key_value_current_level)

                combined_output = torch.cat([attention_output, inter_level_output], dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren
                current_level_input = self.level_projections[i](combined_output)
                current_level_input = self.relu(current_level_input) # ReLU Aktivierungsfunktion nutzen

            # **Neu: Dynamisches Ebenenwachstum basierend auf "Komplexit√§t" oder "Unsicherheit"**
            if i == self.num_layers - 1: # Nach der obersten Ebene pr√ºfen, ob weitere Ebene ben√∂tigt
                complexity_metric = self.calculate_complexity(attention_output) # Funktion zur Messung der Komplexit√§t/Unsicherheit (muss implementiert werden)
                if complexity_metric > self.layer_growth_threshold:
                    self.add_layer() # F√ºge dynamisch eine neue Ebene hinzu

        return level_outputs[-1] # Output der obersten Ebene

    def calculate_complexity(self, level_output):
        """
        Misst die "Komplexit√§t" oder "Unsicherheit" des Outputs einer Ebene.
        Aktuelle Implementierung: Varianz des Outputs.
        """
        # Beispiel: Varianz des Outputs als Komplexit√§tsma√ü
        return torch.var(level_output).item() # Varianz als Beispiel

    def add_layer(self):
        """F√ºgt dynamisch eine neue Abstraktionsebene hinzu."""
        embed_dim = self.attention_layers[-1].attention.embed_dim
        num_heads = self.attention_layers[-1].attention.num_heads
        new_attention_layer = AttentionMechanism(embed_dim, num_heads) # Parameter der letzten Ebene √ºbernehmen
        self.attention_layers.append(new_attention_layer)

        # Projektionen und Inter-Level-Attention nur hinzuf√ºgen, wenn es MEHR als eine Ebene gibt
        if self.num_layers > 0:
            self.level_projections.append(nn.Linear(embed_dim * 2, embed_dim).float()) # Float-Typ
            self.inter_level_attention.append(AttentionMechanism(embed_dim, num_heads))

        self.num_layers += 1
        print(f"INFO: Dynamisch neue Abstraktionsebene hinzugef√ºgt. Aktuelle Ebenenanzahl: {self.num_layers}")


# üéØ Ziel-Klasse
class Goal:
    def __init__(self, name, priority="mittel", subgoals=None): # Subziele hinzugef√ºgt
        self.name = name
        self.priority = priority # "hoch", "mittel", "niedrig"
        self.status = "aktiv" # "aktiv", "inaktiv", "erreicht"
        self.subgoals = subgoals if subgoals is not None else [] # Liste von Subzielen (Strings)

    def __str__(self):
        return f"Ziel: {self.name} (Priorit√§t: {self.priority}, Status: {self.status}, Subziele: {self.subgoals})"

# üé≠ **Neu: Emotionsmodell - ERWEITERT um Dominanz**
class EmotionModel(nn.Module):
    def __init__(self, emotion_dim=3): # Beispiel: 3 dimensionale Emotionsraum (Valenz, Arousal, Dominanz)
        super().__init__()
        self.emotion_dim = emotion_dim
        self.emotions = nn.Parameter(torch.zeros(emotion_dim).float()) # Emotionen als Parameter, initialisiert mit Null

    def update_emotions(self, reward, predictability): # Predictability als Input
        """Aktualisiert Emotionen basierend auf Belohnung UND Predictability."""
        # **Erweiterte Emotionsaktualisierung:**
        # Valenz: Reaktion auf Belohnung (wie zuvor)
        # Fehler behoben: In-place Operation durch out-of-place ersetzt
        self.emotions = nn.Parameter(torch.tensor([self.emotions[0] + reward * 0.1, self.emotions[1], self.emotions[2]])) # Valenz-Dimension (angenommen Dimension 0 ist Valenz)
        self.emotions.data[0] = torch.clamp(self.emotions.data[0], -1.0, 1.0) # Valenz begrenzen (-1 bis 1)

        # Arousal (Erregung/Intensit√§t) basierend auf Predictability (wie zuvor)
        arousal_intensity = 1.0 - predictability # Je niedriger Predictability, desto h√∂her Arousal
        self.emotions.data[1] += arousal_intensity * 0.05 # Arousal-Dimension (angenommen Dimension 1 ist Arousal)
        self.emotions.data[1] = torch.clamp(self.emotions.data[1], 0.0, 1.0) # Arousal begrenzen (0 bis 1)
        self.emotions.data[1] *= 0.99 # Langsamer, nat√ºrlicher "Decay" von Arousal √ºber die Zeit (Vergessen von Aufregung)

        # **Neu: Dominanz (Kontrolle/Macht) basierend auf Belohnung und Predictability (Beispielhaft)**
        # Hohe Belohnung UND hohe Predictability -> Gef√ºhl von Dominanz/Kontrolle verst√§rken
        dominance_intensity = (reward + predictability) / 2.0 # Kombinierter Faktor (vereinfacht)
        self.emotions.data[2] += dominance_intensity * 0.05 # Dominanz-Dimension (angenommen Dimension 2 ist Dominanz)
        self.emotions.data[2] = torch.clamp(self.emotions.data[2], -1.0, 1.0) # Dominanz begrenzen (-1 bis 1)
        self.emotions.data[2] *= 0.99 # Langsamer Decay f√ºr Dominanz


    def get_emotions(self):
        """Gibt den aktuellen Emotionszustand zur√ºck."""
        return self.emotions

# **NEU: Selbstmodell - Repr√§sentation des Agenten SELBST**
class SelfModel:
    def __init__(self):
        """Initialisiert das Selbstmodell."""
        self.current_goal_key = "explore" # Startziel
        self.current_subgoal = None # Aktuelles Subziel (wird sp√§ter gesetzt)
        self.drives = {
            "curiosity": random.uniform(0.1, 0.5),
            "understanding": random.uniform(0.1, 0.3),
            "frustration": random.uniform(0.0, 0.2)
        }
        self.emotions = torch.zeros(3).float() # Initialer Emotionszustand (Valenz, Arousal, Dominanz)
        self.self_awareness_vector = torch.randn(32).float() # Beispiel: Abstrakter Selbstbewusstseins-Vektor

    def update_goal(self, goal_key, subgoal=None):
        """Aktualisiert das aktuelle Ziel und Subziel im Selbstmodell."""
        self.current_goal_key = goal_key
        self.current_subgoal = subgoal

    def update_drives(self, curiosity=None, understanding=None, frustration=None):
        """Aktualisiert die Drives im Selbstmodell."""
        if curiosity is not None: self.drives["curiosity"] = curiosity
        if understanding is not None: self.drives["understanding"] = understanding
        if frustration is not None: self.drives["frustration"] = frustration

    def update_emotions(self, emotions):
        """Aktualisiert den Emotionszustand im Selbstmodell."""
        self.emotions = emotions

    def update_self_awareness(self, awareness_signal):
        """Aktualisiert den Selbstbewusstseins-Vektor (Beispielhaft)."""
        # Hier k√∂nnte komplexere Logik stehen, z.B. basierend auf Reflexion, Koh√§renz, etc.
        self.self_awareness_vector += awareness_signal * 0.1
        self.self_awareness_vector = torch.clamp(self.self_awareness_vector, -1.0, 1.0) # Begrenzen

    def get_self_state(self):
        """Gibt den aktuellen Zustand des Selbstmodells als Dictionary zur√ºck."""
        return {
            "current_goal_key": self.current_goal_key,
            "current_subgoal": self.current_subgoal,
            "drives": self.drives,
            "emotions": self.emotions,
            "self_awareness_vector": self.self_awareness_vector
        }


# ü§ñ KI-Agent mit RL, Feedback-Loop, Hierarchischer Selbst-Attention, Drives, Weltmodell & Zielen
class KI_Agent(nn.Module): # Annahme: KI_Agent Klasse aus vorherigem Code ist vorhanden
    def __init__(self, config, workspace): # **Neu: Workspace als Parameter hinzugef√ºgt**
        super().__init__()

        # **Konfiguration aus Dictionary extrahieren (oder Defaultwerte nutzen)**
        input_size = config.get('input_size', 3) # Default: 3
        action_size = config.get('action_size', 2) # Default: 2
        embed_dim = config.get('embed_dim', 32) # Default: 32
        num_heads = config.get('num_heads', 4) # Default: 4
        state_similarity_threshold = config.get('state_similarity_threshold', 0.5) # Default: 0.5
        emotion_dim = config.get('emotion_dim', 3) # Default: 3
        learning_rate = config.get('learning_rate', 0.01) # Default: 0.01
        memory_capacity = config.get('memory_capacity', 1000) # Default: 1000
        causal_capacity = config.get('causal_capacity', 100) # Default: 100
        prediction_model_hidden_size = config.get('prediction_model_hidden_size', 16) # Default: 16
        layer_growth_threshold = config.get('layer_growth_threshold', 0.9) # Default: 0.9
        novelty_threshold = config.get('novelty_threshold', 0.5) # Default: 0.5
        novelty_tolerance = config.get('novelty_tolerance', 0.5) # Default: 0.5
        reward_history_window = config.get('reward_history_window', 10) # Default: 10


        # **Callbacks aus Konfiguration (falls vorhanden)**
        self.pre_decide_hook = config.get('pre_decide_hook') # Callback vor decide()
        self.post_learn_hook = config.get('post_learn_hook') # Callback nach learn()
        self.rule_engine_hook = config.get('rule_engine_hook') # Callback f√ºr Rule Engine


        # **Modell-Definition (bleibt weitgehend gleich, nutzt konfigurierte Parameter)**
        self.model = nn.Sequential(
            nn.Linear(input_size + emotion_dim, 32), # **Emotion Dim HINZUGEF√úGT als Input-Dimension**
            nn.ReLU(),
            nn.Linear(32, action_size),
            nn.Softmax(dim=-1)
        ).float() # Stelle sicher, dass das Modell Float verwendet
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()

        # **Komponenten initialisieren (nutzt konfigurierte Parameter)**
        self.memory = Memory(capacity=memory_capacity, causal_capacity=causal_capacity) # Konfigurierbare Memory-Kapazit√§t
        self.state_similarity_threshold = state_similarity_threshold # Schwellwert speichern

        self.hierarchical_attention = DynamicHierarchicalSelfAttention(embed_dim, num_heads, layer_growth_threshold=layer_growth_threshold).float() # Nutze DynamicHierarchicalSelfAttention, Float-Typ, konfigurierbarer Schwellwert
        self.input_size = input_size # Speichere input_size f√ºr create_abstract_state
        self.novelty_threshold = novelty_threshold # Schwellwert f√ºr Neuigkeitsbonus
        self.novelty_tolerance = novelty_tolerance # Toleranz f√ºr √Ñhnlichkeit im Neuigkeitsbonus
        self.embed_dim = embed_dim # Wichtig: Speichere embed_dim hier

        # **Neu: Weltmodell (angepasst f√ºr state_size & action_size & konfigurierbare Parameter)**
        self.world_model = WorldModel(state_size=input_size, action_size=1, prediction_model_hidden_size=prediction_model_hidden_size, learning_rate=learning_rate) # state_size und action_size √ºbergeben, action_size korrigiert zu 1, konfigurierbare Parameter

        # **Neu: Ziele (Objekte) mit Subzielen**
        self.goals = {
            "explore": Goal("Erkunde die Welt", priority="hoch", subgoals=["Finde einen neuen Zustand", "Besuche unvorhersehbaren Zustand", "Besuche Zustand mit positiver Valenz"]), # Ziel mit Subzielen
            "reduce_frustration": Goal("Reduziere Frustration", priority="mittel") # Ziel ohne explizite Subziele (f√ºrs Erste)
        }
        self.current_subgoal_index = 0 # Index des aktuellen Subziels (f√ºr "Erkunde die Welt")

        # **Neu: Emotionsmodell instanziieren**
        self.emotion_model = EmotionModel(emotion_dim=emotion_dim) # Emotionsmodell initialisieren
        self.emotion_dim = emotion_dim # Speichere emotion_dim

        # **NEU: Selbstmodell instanziieren**
        self.self_model = SelfModel()
        self.current_goal_key = self.self_model.current_goal_key # Ziel aus Selbstmodell
        self.current_goal = self.goals[self.current_goal_key] # Ziel-Objekt setzen
        self.current_subgoal = self.get_current_subgoal() # Subziel setzen
        self.last_action = 0 # Letzte Aktion initialisieren
        self.last_reward = 0 # Letzte Belohnung initialisieren

        # **Direkter Zugriff auf Komponenten erm√∂glichen (wichtig f√ºr Erweiterbarkeit!)**
        self.config = config # Konfiguration speichern f√ºr evtl. sp√§teren Zugriff
        # Komponenten als √∂ffentliche Attribute zug√§nglich machen
        self.memory_component = self.memory
        self.world_model_component = self.world_model
        self.rule_engine_component = workspace.get_rule_engine() # Zugriff auf Rule Engine √ºber Workspace (wie zuvor)
        self.hierarchical_attention_component = self.hierarchical_attention
        self.emotion_model_component = self.emotion_model
        self.self_model_component = self.self_model


    def get_current_subgoal(self):
        """Ruft das aktuelle Subziel basierend auf dem Index ab."""
        if self.current_goal_key == "explore" and self.current_goal.subgoals: # Nur f√ºr "Erkunde die Welt" mit Subzielen
            return self.current_goal.subgoals[self.current_subgoal_index]
        return None # Kein Subziel f√ºr andere Ziele oder wenn keine Subziele definiert


    def decide(self, state):
        """Agent entscheidet basierend auf aktuellem Zustand, Emotionen & Belohnung in kausaler Erinnerung."""
        # **Callback vor der Entscheidung (pre_decide_hook)**
        if self.pre_decide_hook:
            self.pre_decide_hook(self, state) # Agent-Instanz und Zustand √ºbergeben

        emotion_state_tensor = self.emotion_model.get_emotions() # Aktuellen Emotionszustand abrufen (als Tensor)
        valence = emotion_state_tensor[0].item() # Valenz extrahieren (Dimension 0)
        arousal = emotion_state_tensor[1].item() # Arousal extrahieren (Dimension 1)
        dominance = emotion_state_tensor[2].item() # Dominanz extrahieren (Dimension 2)
        print(f"üòä Emotionen (Valenz: {valence:.2f}, Arousal: {arousal:.2f}, Dominanz: {dominance:.2f})") # Ausgabe der Emotionen


        causal_memories = [self.memory.recall_causal() for _ in range(5)] # Mehrere Erinnerungen abrufen
        relevant_memories = [
            mem for mem in causal_memories
            if mem and self.state_similarity(mem[0], state) < self.state_similarity_threshold
        ]
        action_rewards = {} # Dictionary f√ºr summierte Belohnungen
        if relevant_memories:
            for mem in relevant_memories:
                action, reward = mem[1], mem[3] # Belohnung aus Erinnerung holen
                action_rewards[action] = action_rewards.get(action, 0) + reward # Belohnung summieren

            if action_rewards:
                best_action = max(action_rewards, key=action_rewards.get) # Aktion mit h√∂chster Belohnung
                print(f"üí° Kausale Erinnerung (belohnungsbasiert) beeinflusst Entscheidung! Bevorzugte Aktion: {best_action}")
                # **Epsilon-Greedy: Zuf√§llige Aktion mit Wahrscheinlichkeit epsilon**
                epsilon = 0.1 # Exploration Rate
                if random.random() < epsilon:
                    random_action = random.choice(range(self.model[2].out_features)) # Zuf√§llige Aktion aus Aktionsraum
                    print(f"Œµ-greedy Exploration: Zuf√§llige Aktion gew√§hlt: {random_action}")
                    return random_action
                return best_action

        # **Erweiterung: Emotionen als INPUT f√ºr das neuronale Netz**
        state_tensor = torch.tensor(state, dtype=torch.float32)
        # **Neu: Emotionszustand zum Input-Zustand hinzuf√ºgen**
        emotion_state_tensor = self.emotion_model.get_emotions() # Aktuelle Emotionen abrufen
        model_input = torch.cat([state_tensor, emotion_state_tensor], dim=-1) # Emotionen konkatenieren
        action_probs = self.model(model_input) # **Input enth√§lt nun auch Emotionen**
        nn_action = torch.argmax(action_probs).item()
        print(f"üß† Neuronales Netz entscheidet (mit Emotionen als Input): Aktion: {nn_action}")
        # **Epsilon-Greedy: Zuf√§llige Aktion mit Wahrscheinlichkeit epsilon**
        epsilon = 0.1 # Exploration Rate
        if random.random() < epsilon:
            random_action = random.choice(range(self.model[2].out_features)) # Zuf√§llige Aktion aus Aktionsraum
            print(f"Œµ-greedy Exploration: Zuf√§llige Aktion gew√§hlt: {random_action}")
            return random_action
        return nn_action


    def reflect(self, abstract_state): # Beispiel f√ºr Reflektionsfunktion, die abstrakten Zustand verarbeitet
        """Reflektiert √ºber den abstrakten Zustand mit hierarchischer Aufmerksamkeit."""
        abstract_state = abstract_state.float() # Stelle sicher, dass abstract_state Float ist
        processed_state = self.hierarchical_attention(abstract_state)
        return processed_state

    def learn(self, state, action, reward, next_state):
        #super().learn(state, action, reward) # Basis-Lernprozess beibehalten

        target = torch.zeros(2)
        target[action] = reward
        state_tensor = torch.tensor(state, dtype=torch.float32)

        self.optimizer.zero_grad()
        # **Lernprozess angepasst an neuen Input des Modells (mit Emotionen)**
        emotion_state_tensor = self.emotion_model.get_emotions() # Aktuelle Emotionen abrufen
        model_input = torch.cat([state_tensor, emotion_state_tensor], dim=-1) # Input f√ºr Modell erstellen (Zustand + Emotionen)
        output = self.model(model_input) # **Modell-Input enth√§lt nun Emotionen**
        loss = self.loss_fn(output, target)
        loss.backward()
        self.optimizer.step()

        # Kausale Erinnerung speichern (mit next_state und reward)
        self.memory.store_causal(state, action, next_state, reward)

        # Last Action/Reward speichern
        self.last_action = action
        self.last_reward = reward

        # **Neu: Weltmodell aktualisieren (inkl. Training des Prediction Models)**
        next_state_wm = np.random.rand(self.input_size) # Vereinfachung: N√§chster Zustand = zuf√§llige Wahrnehmung (f√ºr Demo)
        self.world_model.add_transition(state, action, next_state_wm, reward) # Weltmodell aktualisieren, inkl. Training

        # Zustandseigenschaften berechnen (f√ºr Emotionsmodell-Update)
        state_properties = self.world_model.calculate_state_properties(state, action) # Zustandseigenschaften berechnen (ACTION hinzugef√ºgt!)
        predictability = state_properties.get('predictability', 0.0) # Predictability extrahieren

        # Reflektion nach dem Lernen, um intrinsische Belohnung zu generieren
        abstract_state = self.create_abstract_state() # Funktion zur Erzeugung eines abstrakten Zustands
        reflected_state = self.reflect(abstract_state) # Reflektiere √ºber den abstrakten Zustand

        intrinsic_reward = self.calculate_intrinsic_reward(abstract_state, reflected_state) # Berechne intrinsische Belohnung
        total_reward = reward + intrinsic_reward # Kombiniere extrinsische und intrinsische Belohnung

        # Erweitere Lernfunktion, um intrinsische Belohnung zu nutzen (Beispielhaft)
        target_intrinsic = torch.zeros(2)
        target_intrinsic[action] = total_reward # Nutze Gesamtbelohnung
        state_tensor_intrinsic = torch.tensor(state, dtype=torch.float32)
        # **Lernprozess f√ºr intrinsische Belohnung angepasst (mit Emotionen im Input)**
        emotion_state_tensor_intrinsic = self.emotion_model.get_emotions() # Aktuelle Emotionen abrufen
        model_input_intrinsic = torch.cat([state_tensor_intrinsic, emotion_state_tensor_intrinsic], dim=-1) # Input erstellen
        output_intrinsic = self.model(model_input_intrinsic) # **Modell-Input enth√§lt Emotionen**
        loss_intrinsic = self.loss_fn(output_intrinsic, target_intrinsic)
        loss_intrinsic.backward()
        self.optimizer.step()

        # Drives aktualisieren (rudiment√§r)
        valence_emotion = self.emotion_model.get_emotions()[0].item() # Valenz f√ºr Drive-Modulation extrahieren
        self.update_drives(intrinsic_reward, valence_emotion) # Valenz √ºbergeben f√ºr Drive-Modulation

        # **Neu: Emotionsmodell aktualisieren (mit Predictability)**
        self.emotion_model.update_emotions(reward, predictability) # Emotionen basierend auf extrinsischer Belohnung & Predictability aktualisieren

        # **NEU: Selbstmodell aktualisieren (nach allen anderen Updates)**
        self.update_self_model()

        # **Callback nach dem Lernen (post_learn_hook)**
        if self.post_learn_hook:
            self.post_learn_hook(self, state, action, reward, next_state) # Agent-Instanz und Lernparameter √ºbergeben


    def update_self_model(self):
        """Aktualisiert das Selbstmodell mit aktuellen Agenten-Zust√§nden."""
        # Drives direkt aus dem Selbstmodell holen
        self.self_model.update_drives(
            curiosity=self.self_model.drives["curiosity"],
            understanding=self.self_model.drives["understanding"],
            frustration=self.self_model.drives["frustration"]
        )
        self.self_model.update_emotions(self.emotion_model.get_emotions())
        self.self_model.update_goal(self.current_goal_key, self.current_subgoal) # Ziel & Subziel auch updaten
        # **Zukunft:** Hier k√∂nnte man komplexere Selbst-Reflexions-Prozesse einbauen, um das Selbstmodell zu aktualisieren.
        # Z.B. basierend auf Erfolg/Misserfolg bei Zielen, Ver√§nderungen in Drives/Emotionen, etc.
        # F√ºrs Erste: Einfache √úbernahme der aktuellen Werte.


    def create_abstract_state(self):
        """
        Erzeugt einen abstrakten Zustand aus dem internen Ged√§chtnis, aktuellen Wahrnehmungen, Drives, Weltmodell (rudiment√§r) & Ziel & Subziel & **Emotionen & Selbstmodell**.
        """
        # Beispiel: Kombiniere zuf√§llige Erinnerungen, aktuellen Zustand und Drives zu einem abstrakten Zustand
        recalled_memory = self.memory.recall()
        current_perception = torch.tensor(np.random.rand(self.input_size), dtype=torch.float32).unsqueeze(0) # Aktuelle Wahrnehmung simulieren, Batch-Dimension hinzuf√ºgen
        #drive_state = torch.tensor([self.curiosity_drive, self.understanding_drive, self.frustration_drive]).unsqueeze(0) # Drive-Zust√§nde als Tensor, Batch-Dimension hinzuf√ºgen
        world_model_state = torch.tensor([self.last_action, self.last_reward]).unsqueeze(0) # Letzte Aktion und Belohnung (rudiment√§res Weltmodell)
        goal_state = torch.tensor([1.0 if self.current_goal_key == "explore" else 0.0, 1.0 if self.current_goal_key == "reduce_frustration" else 0.0]).unsqueeze(0) # Ziel als One-Hot √ºber Key
        subgoal_state = torch.tensor([1.0 if self.current_subgoal == "Finde einen neuen Zustand" else 0.0, 1.0 if self.current_subgoal == "Besuche unvorhersehbaren Zustand" else 0.0, 1.0 if self.current_subgoal == "Besuche Zustand mit positiver Valenz" else 0.0]).unsqueeze(0) # Subziel als One-Hot (rudiment√§r)

        # **Neu: √Ñhnlichen Zustand aus Weltmodell abrufen (mit Eigenschaften)**
        similar_world_state, state_properties = self.world_model.recall_similar_state(current_perception.squeeze(0).numpy(), tolerance=0.8) # Zustandsteil der Erinnerung nutzen (vereinfacht), Batch-Dimension hinzuf√ºgen
        if similar_world_state is None:
            similar_world_state = torch.zeros_like(current_perception) # Wenn kein √§hnlicher Zustand, Null-Tensor
            state_properties = {} # Leere Eigenschaften, wenn kein Zustand gefunden

        # **Neu: Zustandseigenschaften in abstrakten Zustand integrieren (rudiment√§r)**
        novelty_prop = torch.tensor([state_properties.get('novelty_score', 0.0)]).unsqueeze(0) # Neuigkeitswert extrahieren, default 0.0
        frustration_prop = torch.tensor([state_properties.get('frustration_level', 0.0)]).unsqueeze(0) # Frustrationslevel extrahieren, default 0.0
        predictability_prop = torch.tensor([state_properties.get('predictability', 0.0)]).unsqueeze(0) # Predictability extrahieren, default 0.0
        valence_prop = torch.tensor([state_properties.get('valence', 0.0)]).unsqueeze(0) # Valence extrahieren, default 0.0

        # **Neu: Emotionszustand hinzuf√ºgen (jetzt Valenz & Arousal & Dominanz)**
        emotion_state = self.emotion_model.get_emotions().unsqueeze(0) # Emotionszustand abrufen und Batch-Dimension hinzuf√ºgen

        # **NEU: Drives aus dem Selbstmodell holen**
        drive_state = torch.tensor(list(self.self_model.drives.values())).unsqueeze(0) # Drives aus Selbstmodell, Batch-Dimension

        # **NEU: Selbstbewusstseins-Vektor aus Selbstmodell holen**
        self_awareness_vector = self.self_model.self_awareness_vector.unsqueeze(0) # Selbstbewusstseins-Vektor, Batch-Dimension


        if recalled_memory:
            memory_tensor = torch.tensor(recalled_memory[0], dtype=torch.float32).unsqueeze(0) # Nur den Zustandsteil der Erinnerung nutzen (vereinfacht), Batch-Dimension hinzuf√ºgen
            abstract_state_list = [current_perception, memory_tensor, drive_state, world_model_state, goal_state, subgoal_state, similar_world_state, novelty_prop, frustration_prop, predictability_prop, valence_prop, emotion_state, self_awareness_vector] # Emotionszustand + Selbstmodell hinzugef√ºgt
        else:
            abstract_state_list = [current_perception, drive_state, world_model_state, goal_state, subgoal_state, similar_world_state, novelty_prop, frustration_prop, predictability_prop, valence_prop, emotion_state, self_awareness_vector] # Emotionszustand + Selbstmodell hinzugef√ºgt

        abstract_state = torch.cat(abstract_state_list, dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren
        abstract_state = abstract_state.float() # Stelle sicher, dass abstract_state Float ist

        # Padding, um sicherzustellen, dass die Dimension immer embed_dim ist
        current_dim = abstract_state.shape[1]
        if current_dim < self.embed_dim:
            padding_size = self.embed_dim - current_dim
            padding = torch.zeros(abstract_state.shape[0], padding_size) # Batch-Dimension beibehalten
            abstract_state = torch.cat([abstract_state, padding], dim=-1) # Korrektur: dim=-1 f√ºr horizontales Konkatenieren
        elif current_dim > self.embed_dim:
            # Fallback: Dimension reduzieren, falls sie gr√∂√üer ist (sollte nicht passieren, aber sicherheitshalber)
            abstract_state = abstract_state[:, :self.embed_dim]

        return abstract_state


    def calculate_intrinsic_reward(self, abstract_state, reflected_state):
        """
        Berechnet die intrinsische Belohnung basierend auf Informationsgewinn, Neuigkeit und Koh√§renz, beeinflusst von Drives.
        """
        information_gain = self.calculate_information_gain(abstract_state, reflected_state)
        novelty_bonus = self.calculate_novelty_bonus(abstract_state)
        coherence_reward = self.calculate_coherence_reward(reflected_state)

        # Drives beeinflussen Belohnungsgewichtungen (dynamischer)
        # **NEU: Drives aus Selbstmodell holen**
        curiosity_drive = self.self_model.drives["curiosity"]
        understanding_drive = self.self_model.drives["understanding"]

        novelty_weight = 0.2 + curiosity_drive * 0.8 # Neugier erh√∂ht Gewichtung von Neuigkeit (st√§rker)
        coherence_weight = 0.1 + understanding_drive * 0.7 # Verst√§ndnis-Drive erh√∂ht Gewichtung von Koh√§renz (st√§rker)
        information_gain_weight = 1.0 - novelty_weight - coherence_weight # Rest f√ºr Informationsgewinn

        # Kombiniere die Belohnungskomponenten (dynamische Gewichtungen basierend auf Drives)
        intrinsic_reward = information_gain_weight * information_gain + novelty_weight * novelty_bonus + coherence_weight * coherence_reward
        return intrinsic_reward

    def calculate_information_gain(self, state_before, state_after):
        """Misst den Informationsgewinn (Beispiel: Distanz√§nderung)."""
        return torch.norm(state_after - state_before).item() # Beispiel: Euklidische Distanz

    def calculate_novelty_bonus(self, state):
        """Berechnet den Neuigkeits-Bonus (Beispiel: Seltenheit im Ged√§chtnis)."""
        # Vereinfachtes Beispiel: Z√§hle Vorkommen √§hnlicher Zust√§nde im Ged√§chtnis
        novelty_count = 0
        for memory in self.memory.memory:
            if memory and torch.allclose(torch.tensor(memory[0]), state.squeeze(0), atol=self.novelty_tolerance): # Zustandsteil der Erinnerung vergleichen (Toleranz), Batch-Dimension entfernen
                novelty_count += 1

        # Neu: Bonus nur, wenn Zustand "neu genug" ist
        if novelty_count < self.novelty_threshold * self.memory.capacity: # Beispiel: Bonus, wenn Zustand seltener als Schwellwert * Kapazit√§t
            return 1.0 / (novelty_count + 1) # Je seltener, desto h√∂her der Bonus
        else:
            return 0.0 # Kein Bonus f√ºr "bekannte" Zust√§nde


    def calculate_coherence_reward(self, reflected_state):
        """Berechnet die Koh√§renz-Belohnung (Beispiel: Innere Konsistenz)."""
        # Sehr vereinfachtes Beispiel:  Annahme: Reflektierter Zustand sollte "einfacher" oder "kompakter" sein
        # -> Messung der Norm des reflektierten Zustands als Indikator f√ºr "Einfachheit/Koh√§renz"
        return -torch.norm(reflected_state).item() # Negative Norm -> Minimierung der Norm wird belohnt (vereinfacht!)

    def update_drives(self, intrinsic_reward, valence_emotion): # Valenz als Parameter
        """Aktualisiert die Drives basierend auf intrinsischer Belohnung, Emotionen, Zufall und Frustration."""
        # Rudiment√§re Drive-Aktualisierung (Beispielhaft, inkl. Frustration)
        # **NEU: Drives werden DIREKT im Selbstmodell aktualisiert (und von dort in create_abstract_state geholt)**
        current_curiosity = self.self_model.drives["curiosity"]
        current_understanding = self.self_model.drives["understanding"]
        current_frustration = self.self_model.drives["frustration"]

        updated_curiosity = current_curiosity + intrinsic_reward * 0.02 + random.uniform(-0.01, 0.02) # Durch Belohnung verst√§rken, zuf√§llige Fluktuation
        updated_understanding = current_understanding + intrinsic_reward * 0.01 + random.uniform(-0.005, 0.005) # Durch Belohnung verst√§rken, zuf√§llige Fluktuation
        updated_frustration = current_frustration

        # **Neu: Emotionale Modulation der Drives (rudiment√§r)**
        # Negative Valenz -> Frustration verst√§rken, Neugier & Verst√§ndnis d√§mpfen (Beispielhaft)
        if valence_emotion < -0.2:
            updated_frustration += 0.03 # Frustration st√§rker erh√∂hen bei negativer Valenz
            updated_curiosity -= 0.02    # Neugier leicht d√§mpfen
            updated_understanding -= 0.01 # Verst√§ndnis leicht d√§mpfen
            print(f"   -> Emotionale Drive-Modulation (negative Valenz): Frustration++, Neugier--, Verst√§ndnis--")
        elif valence_emotion > 0.3: # Positive Valenz -> Neugier & Verst√§ndnis verst√§rken (Beispielhaft)
            updated_curiosity += 0.03     # Neugier leicht verst√§rken
            updated_understanding += 0.02  # Verst√§ndnis leicht verst√§rken
            updated_frustration -= 0.01    # Frustration leicht d√§mpfen
            print(f"   -> Emotionale Drive-Modulation (positive Valenz): Neugier++, Verst√§ndnis++, Frustration--")


        # Frustration anpassen: Steigt bei niedriger Neuigkeit/Koh√§renz, sinkt leicht zuf√§llig (wie zuvor)
        novelty_bonus = self.calculate_novelty_bonus(self.create_abstract_state()) # Neuigkeitsbonus aktuell berechnen (etwas ineffizient, aber f√ºr Demo ok)
        coherence_reward = self.calculate_coherence_reward(self.reflect(self.create_abstract_state())) # Koh√§renz auch
        if novelty_bonus < 0.1 or coherence_reward < -0.8: # Beispielschwellwerte f√ºr Frustration
            updated_frustration += 0.05 # Frustration steigt
        else:
            updated_frustration -= 0.01 # Frustration sinkt leicht (Baseline-Abbau)

        updated_frustration = np.clip(updated_frustration, 0.0, 1.0) # Frustration begrenzen
        updated_curiosity = np.clip(updated_curiosity, 0.0, 1.0) # Drives begrenzen
        updated_understanding = np.clip(updated_understanding, 0.0, 1.0)

        # **NEU: Drives im Selbstmodell aktualisieren**
        self.self_model.update_drives(curiosity=updated_curiosity, understanding=updated_understanding, frustration=updated_frustration)

        # Beispiel: Ausgabe der Drive-Zust√§nde (f√ºr Beobachtung)
        print(f"INFO: Drives aktualisiert - Neugier: {updated_curiosity:.3f}, Verst√§ndnis: {updated_understanding:.3f}, Frustration: {updated_frustration:.3f}")
        print(f"      Intrinsische Belohnung: {intrinsic_reward:.3f}, Valenz: {valence_emotion:.3f}")

        # **Neu: Ziel- & Subziel-√úberpr√ºfung & -Anpassung (rudiment√§r)**
        self.check_and_update_goal()


    def check_and_update_goal(self):
        """√úberpr√ºft das aktuelle Ziel und √§ndert es ggf. basierend auf Agenten-Zustand (rudiment√§r), inkl. Subziel-Management."""
        if self.current_goal_key == "reduce_frustration": # Ziel-Key nutzen
            if self.self_model.drives["frustration"] < 0.3: # Beispielschwellwert f√ºr Frustration reduziert
                print(f"‚úÖ Ziel erreicht: Frustration reduziert ({self.goals['reduce_frustration'].name}). Neues Ziel: Erkunde die Welt.") # Zielnamen aus Objekt
                self.current_goal_key = "explore" # Ziel-Key wechseln
                self.current_goal = self.goals[self.current_goal_key] # Ziel-Objekt aktualisieren
                self.goals["reduce_frustration"].status = "erreicht" # Status des alten Ziels √§ndern
                self.goals["explore"].status = "aktiv" # Status des neuen Ziels √§ndern
                self.current_subgoal_index = 0 # Subziel-Index zur√ºcksetzen f√ºr neues Hauptziel
                self.current_subgoal = self.get_current_subgoal() # Subziel aktualisieren
                # **NEU: Selbstmodell aktualisieren mit neuem Ziel**
                self.self_model.update_goal(self.current_goal_key, self.current_subgoal)


        elif self.current_goal_key == "explore": # Ziel-Key nutzen
            if self.self_model.drives["frustration"] > 0.7: # Wenn Frustration beim Erkunden zu hoch wird...
                print(f"‚ö†Ô∏è Frustration beim Erkunden zu hoch ({self.goals['explore'].name}). Zielwechsel: Reduziere Frustration.") # Zielnamen aus Objekt
                self.current_goal_key = "reduce_frustration" # Ziel-Key wechseln
                self.current_goal = self.goals[self.current_goal_key] # Ziel-Objekt aktualisieren
                self.goals["explore"].status = "inaktiv" # Status des alten Ziels √§ndern
                self.goals["reduce_frustration"].status = "aktiv" # Status des neuen Ziels √§ndern
                self.current_subgoal_index = 0 # Subziel-Index zur√ºcksetzen f√ºr neues Hauptziel
                self.current_subgoal = self.get_current_subgoal() # Subziel aktualisieren
                # **NEU: Selbstmodell aktualisieren mit neuem Ziel**
                self.self_model.update_goal(self.current_goal_key, self.current_subgoal)

            else: # **Neu: Subziel-Management innerhalb von "Erkunde die Welt"**
                if self.current_subgoal == "Finde einen neuen Zustand":
                    novelty_bonus = self.calculate_novelty_bonus(self.create_abstract_state())
                    if novelty_bonus < 0.1: # Wenn wenig Neues gefunden... (Beispielschwellwert)
                        print(f"Subziel '{self.current_subgoal}' (Ziel: {self.current_goal.name}) scheint schwierig. Wechsle Subziel.")
                        self.current_subgoal_index = (self.current_subgoal_index + 1) % len(self.current_goal.subgoals) # Zum n√§chsten Subziel wechseln (zyklisch)
                        self.current_subgoal = self.get_current_subgoal() # Subziel aktualisieren
                        print(f"Neues Subziel: '{self.current_subgoal}' (Ziel: {self.current_goal.name})")
                        # **NEU: Selbstmodell aktualisieren mit neuem Subziel**
                        self.self_model.update_goal(self.current_goal_key, self.current_subgoal)


    def state_similarity(self, state1, state2):
        """Berechnet die euklidische Distanz zwischen zwei Zust√§nden."""
        return np.linalg.norm(np.array(state1) - np.array(state2))


# üåê Globale Regel-Engine (zuk√ºnftig f√ºr inneren Dialog)
class RuleEngine:
    def __init__(self):
        # Hier werden sp√§ter Regeln f√ºr den inneren Dialog gespeichert
        pass

    def apply_rules(self, agent_state, global_workspace, agent): # Zugriff auf Agent ben√∂tigt
        """
        Wendet Regeln an, um den Informationsfluss und das Verhalten des Agenten zu steuern.
        Erweitert um zielbezogene Regeln, Weltmodell-Nutzung & **Selbst-Reflexions-Regeln** & **Emotions-basierte Regeln & **Selbstmodell-basierte Regeln**.
        """
        novelty_bonus = agent_state.get('novelty_bonus', 0.0)
        coherence_reward = agent_state.get('coherence_reward', 0.0)
        frustration_drive = agent_state.get('frustration_drive', 0.0)
        complexity_metric = agent_state.get('complexity_metric', 0.0)
        predictability = agent_state.get('predictability', 0.0) # Neu: Predictability
        valence = agent_state.get('valence', 0.0) # Neu: Valence
        current_goal_key = agent_state.get('current_goal_key', "Kein Ziel") # Aktueller Ziel-Key abrufen
        current_subgoal = agent_state.get('current_subgoal', "Kein Subziel") # Aktuelles Subziel abrufen
        emotion_state = agent_state.get('emotion_state', torch.zeros(agent.emotion_dim).tolist()) # Neu: Emotionszustand abrufen
        valence_emotion = emotion_state[0] # Annahme: Erste Dimension ist Valenz
        arousal_emotion = emotion_state[1] # Annahme: Zweite Dimension ist Arousal
        dominance_emotion = emotion_state[2] # Annahme: Dritte Dimension ist Dominanz
        self_awareness_vector = agent_state.get('self_awareness_vector', torch.zeros(32).tolist()) # **NEU**: Selbstbewusstseins-Vektor

        # **Callback f√ºr Rule Engine (rule_engine_hook) - VOR Regeln**
        if agent.rule_engine_hook:
            agent.rule_engine_hook(agent, agent_state, "pre_rules") # Agent-Instanz, Agent-Zustand, Phase √ºbergeben


        # **Ziel-bezogene Regeln (rudiment√§r) - Ziel-KEY nutzen**
        if current_goal_key == "explore": # Ziel-Key nutzen
            if novelty_bonus > 0.8: # Bei hohem Neuigkeitsbonus...
                print(f"üí¨ Regel-Engine (Ziel: Erkunden, Subziel: {current_subgoal}): Neuigkeitsbonus hoch ({novelty_bonus:.2f}) - 'Frage' Ged√§chtnis nach √§hnlichen Erfahrungen...")
                # Neu: Aktion: Gezielter Memory Recall (√§hnliche Erfahrungen)
                similar_memory = agent.memory.recall_similar(agent.create_abstract_state()) # √Ñhnliche Erinnerung abrufen
                if similar_memory:
                    global_workspace.share(f"Regel-Engine-Hinweis (Ziel: Erkunden, Subziel: {current_subgoal}): √Ñhnliche Erinnerung abgerufen: {similar_memory}")

            if complexity_metric > 0.95: # Bei hoher Komplexit√§t... (immer noch relevant)
                print(f"üí¨ Regel-Engine (Ziel: Erkunden, Subziel: {current_subgoal}): Hohe Komplexit√§t ({complexity_metric:.2f}) - Dynamische Ebene hinzuf√ºgen (Beispiel)")
                agent.hierarchical_attention.add_layer() # Dynamisch Ebene hinzuf√ºgen (als Reaktion auf Komplexit√§t)

        elif current_goal_key == "reduce_frustration": # Ziel-Key nutzen
            if frustration_drive > 0.6: # Bei hoher Frustration...
                print(f"üí¨ Regel-Engine (Ziel: Frustration reduzieren): Frustration hoch ({frustration_drive:.2f}) - 'Suche' nach einfacheren Mustern, erh√∂he Toleranz...")
                # Neu: Aktion: Erh√∂he Neuigkeitstoleranz (f√ºr explorativeres Verhalten -> einfachere Muster "akzeptieren")
                agent.novelty_tolerance += 0.2 # Erh√∂he Toleranz st√§rker, um "bekanntere" Zust√§nde als neu zu betrachten
                agent.novelty_tolerance = np.clip(agent.novelty_tolerance, 0.0, 1.0) # Begrenzen
                print(f"   -> Neuigkeitstoleranz erh√∂ht auf {agent.novelty_tolerance:.2f} (Fokus auf Bekanntes)")

            if coherence_reward < -0.7: # Bei SEHR niedriger Koh√§renz (evtl. in frustrierender Situation)...
                print(f"üí¨ Regel-Engine (Ziel: Frustration reduzieren): Koh√§renz SEHR niedrig ({coherence_reward:.2f}) - Setze Neuigkeitstoleranz zur√ºck (Fokus auf Bekanntes)...")
                agent.novelty_tolerance = 0.1 # Setze Toleranz niedriger, um Fokus st√§rker auf Bekanntes zu lenken (Verankerung in bekanntem Terrain)
                print(f"   -> Neuigkeitstoleranz reduziert auf {agent.novelty_tolerance:.2f} (Fokus auf Bekanntes)")

        # Weltmodell-bezogene Regeln (Beispielhaft)
        if novelty_bonus < 0.2 and current_goal_key == "explore": # Wenn wenig Neues entdeckt wird, aber Ziel "Erkunden" ist... (Ziel-Key nutzen)
            print(f"üí¨ Regel-Engine (Ziel: Erkunden, Subziel: {current_subgoal}): Wenig Neues ({novelty_bonus:.2f}) -  '√úberpr√ºfe' Weltmodell nach unerkundeten Pfaden...")
            # **Neu: Aktion: Nutze Weltmodell, um explorativer zu werden (rudiment√§r)**
            similar_state_from_wm = agent.world_model.recall_similar_state(agent.create_abstract_state().squeeze(0).numpy(), tolerance=0.8)[0] # √Ñhnlichen Zustand abrufen, nur Zustandsteil [0]
            if similar_state_from_wm is not None:
                transitions = agent.world_model.get_transitions(similar_state_from_wm.squeeze(0).numpy()) # Transitionen f√ºr √§hnlichen Zustand abrufen
                if transitions:
                    print(f"   -> Weltmodell-Hinweis: Transitionen gefunden f√ºr √§hnlichen Zustand: {transitions.keys()}")
                    # **Vision√§r:** Hier k√∂nnte Agent Aktionen w√§hlen, die zu neuen/unbekannten Zust√§nden f√ºhren (basierend auf Weltmodell)
                    # **Aktuell (rudiment√§r):**  Erh√∂he Neugier-Drive leicht, um Exploration zu f√∂rdern
                    agent.self_model.update_drives(curiosity=agent.self_model.drives["curiosity"] + 0.05) # Neugier im Selbstmodell aktualisieren
                    print(f"      -> Neugier-Drive leicht erh√∂ht auf {agent.self_model.drives['curiosity']:.2f}")

        # **Neu: Selbst-Reflexions-Regeln (rudiment√§r)**
        if frustration_drive > 0.8: # Beispiel-Trigger f√ºr Selbst-Reflexion: Hohe Frustration
            print(f"ü§î Selbst-Reflexion (Frustration hoch): 'Warum bin ich so frustriert?  Was kann ich √§ndern?' (Ziel: {current_goal_key}, Subziel: {current_subgoal})")
            global_workspace.share(f"Selbst-Reflexion: Agent frustriert. Ziel: {current_goal_key}, Subziel: {current_subgoal}, Frustration: {frustration_drive:.2f}") # Workspace-Mitteilung

            if predictability < 0.2: # Wenn Zustand auch noch unvorhersehbar ist...
                print(f"   -> Zustand unvorhersehbar ({predictability:.2f}). 'Vielleicht sollte ich mich auf Vorhersagbareres konzentrieren?'")
                global_workspace.share(f"Selbst-Reflexion: Zustand unvorhersehbar. Predictability: {predictability:.2f}")

        elif coherence_reward < -0.9: # Beispiel-Trigger: Sehr niedrige Koh√§renz
            print(f"ü§î Selbst-Reflexion (Koh√§renz niedrig): 'Meine Gedanken sind unzusammenh√§ngend. Muss Fokus √§ndern?' (Ziel: {current_goal_key}, Subziel: {current_subgoal}, Koh√§renz: {coherence_reward:.2f})")
            global_workspace.share(f"Selbst-Reflexion: Koh√§renz niedrig. Ziel: {current_goal_key}, Subziel: {current_subgoal}, Koh√§renz: {coherence_reward:.2f}")
            # **Rudiment√§re Anpassung: Subziel wechseln (Beispiel)**
            if current_goal_key == "explore": # Nur f√ºr Explore-Ziel (Beispiel)
                print(f"   -> Subziel-Wechsel initiiert (Koh√§renz niedrig).")
                agent.current_subgoal_index = (agent.current_subgoal_index + 1) % len(agent.current_goal.subgoals) # Zum n√§chsten Subziel wechseln (zyklisch)
                agent.current_subgoal = agent.get_current_subgoal() # Subziel aktualisieren
                print(f"   -> Neues Subziel: '{current_subgoal}' (Ziel: {agent.goals['explore'].name})")
                # **NEU: Selbstmodell aktualisieren mit neuem Subziel**
                agent.self_model.update_goal(agent.current_goal_key, agent.current_subgoal)


        # **Neu: Emotions-basierte Regeln (rudiment√§r) - ERWEITERT mit AROUSAL & DOMINANZ**
        if valence_emotion < -0.5 and current_goal_key == "explore": # Negative Valenz beim Erkunden...
            print(f"üòû Regel-Engine (Ziel: Erkunden, Emotion: Negative Valenz ({valence_emotion:.2f}), Arousal: {arousal_emotion:.2f}, Dominanz: {dominance_emotion:.2f}): 'Erkundung ist unangenehm. Fokus √§ndern?'")
            global_workspace.share(f"Regel-Engine-Hinweis: Negative Valenz beim Erkunden. Valenz: {valence_emotion:.2f}, Arousal: {arousal_emotion:.2f}, Dominanz: {dominance_emotion:.2f}")
            # **Rudiment√§re Reaktion: Zielwechsel zu Frustrationsreduktion (Beispiel)**
            print(f"   -> Zielwechsel zu 'Reduziere Frustration' vorgeschlagen (basierend auf negativer Valenz).")
            agent.current_goal_key = "reduce_frustration"
            agent.current_goal = agent.goals[agent.current_goal_key]
            agent.goals["explore"].status = "inaktiv"
            agent.goals["reduce_frustration"].status = "aktiv"
            agent.current_subgoal_index = 0
            agent.current_subgoal = agent.get_current_subgoal()
            # **NEU: Selbstmodell aktualisieren mit neuem Ziel**
            agent.self_model.update_goal(agent.current_goal_key, agent.current_subgoal)


        if arousal_emotion > 0.8 and current_goal_key == "explore": # Hohes Arousal beim Erkunden... (z.B. unerwarteter Zustand)
            print(f"üòÆ Regel-Engine (Ziel: Erkunden, Emotion: Hohes Arousal ({arousal_emotion:.2f}), Valenz: {valence_emotion:.2f}, Dominanz: {dominance_emotion:.2f}): 'Unerwartetes/Aufregendes gefunden! Fokus verst√§rken?'")
            global_workspace.share(f"Regel-Engine-Hinweis: Hohes Arousal beim Erkunden. Arousal: {arousal_emotion:.2f}, Valenz: {valence_emotion:.2f}, Dominanz: {dominance_emotion:.2f}")
            # **Rudiment√§re Reaktion: Neugier-Drive verst√§rken, Subziel anpassen (Beispiel)**
            print(f"   -> Neugier-Drive leicht erh√∂ht, Subziel ggf. anpassen (Fokus auf Unerwartetes?).")
            agent.self_model.update_drives(curiosity=agent.self_model.drives["curiosity"] + 0.1) # Neugier im Selbstmodell aktualisieren
            # **Zuk√ºnftig:** Subziel k√∂nnte auf "Besuche unvorhersehbaren Zustand" wechseln oder verst√§rkt werden.

        # **Neu: Dominanz-basierte Regel (Beispiel)**
        if dominance_emotion > 0.6 and current_goal_key == "reduce_frustration": # Gef√ºhl von Dominanz, w√§hrend Frustration reduziert werden soll...
            print(f"üòé Regel-Engine (Ziel: Frustration reduzieren, Emotion: Hohe Dominanz ({dominance_emotion:.2f}), Valenz: {valence_emotion:.2f}, Arousal: {arousal_emotion:.2f}): 'Ich habe Kontrolle √ºber die Frustration! Ziel √ºberdenken?'")
            global_workspace.share(f"Regel-Engine-Hinweis: Dominanzgef√ºhl bei Frustrationsreduktion. Dominanz: {dominance_emotion:.2f}")
            # **Vision√§r/Provokativ:**  Hier k√∂nnte der Agent *in Frage stellen*, ob das Ziel "Frustration reduzieren" noch sinnvoll ist, wenn er sich dominant/kontrollierend f√ºhlt.
            # **Rudiment√§re Reaktion (Beispiel):**  Priorit√§t des Ziels "Frustration reduzieren" leicht senken, um Exploration wieder zu erm√∂glichen.
            if agent.goals["reduce_frustration"].priority == "mittel":
                agent.goals["reduce_frustration"].priority = "niedrig"
                agent.goals["explore"].priority = "mittel" # Priorit√§t von "Erkunden" leicht erh√∂hen
                print(f"   -> Priorit√§t von 'Frustration reduzieren' auf 'niedrig' gesenkt, 'Erkunden' auf 'mittel' erh√∂ht (basierend auf Dominanzgef√ºhl).")


        # **NEU: Selbstmodell-basierte Regeln (Beispiel)**
        self_awareness_norm = torch.norm(torch.tensor(self_awareness_vector)).item() # Norm des Selbstbewusstseins-Vektors als rudiment√§res Ma√ü
        if self_awareness_norm > 5.0 and current_goal_key == "explore": # Beispielschwellwert f√ºr "hohes Selbstbewusstsein"
            print(f"üåü Regel-Engine (Selbstmodell-basiert, Ziel: Erkunden, Selbstbewusstsein hoch ({self_awareness_norm:.2f})): 'Ich bin mir meiner selbst st√§rker bewusst.  Vielleicht komplexere Exploration?'")
            global_workspace.share(f"Regel-Engine-Hinweis: Selbstbewusstsein hoch beim Erkunden. Selbstbewusstseins-Norm: {self_awareness_norm:.2f}")
            # **Vision√§r:** Hier k√∂nnte der Agent beginnen, komplexere Explorationsstrategien zu entwickeln, basierend auf seinem erh√∂hten Selbstverst√§ndnis.
            # **Rudiment√§re Reaktion (Beispiel):**  Schwellwert f√ºr Neuigkeitsbonus leicht erh√∂hen, um anspruchsvollere neue Zust√§nde zu suchen.
            agent.novelty_threshold += 0.1
            agent.novelty_threshold = np.clip(agent.novelty_threshold, 0.0, 1.0)
            print(f"   -> Neuigkeits-Schwellwert leicht erh√∂ht auf {agent.novelty_threshold:.2f} (anspruchsvollere Exploration)")


        else:
            print("üí¨ Regel-Engine: Keine aktiven Regeln (Standard).")



# üåê Globaler Workspace zur Koordination
class GlobalWorkspace:
    def __init__(self):
        self.data = []
        self.rule_engine = RuleEngine() # Regel-Engine hinzuf√ºgen

    def share(self, info):
        """Daten mit anderen Agenten teilen."""
        self.data.append(info)

    def access(self):
        """Zuf√§llige Information abrufen."""
        if self.data:
            return random.choice(self.data)
        return None

    def get_rule_engine(self): # Zugriff auf die Rule Engine erm√∂glichen
        return self.rule_engine


# **Neu: Funktion zum Laden der YAML-Konfiguration**
def lade_config_aus_yaml(dateipfad):
    """L√§dt die Konfiguration aus einer YAML-Datei."""
    with open(dateipfad, 'r') as file:
        config = yaml.safe_load(file)
    return config

# **Neu: Funktion zum Erstellen eines konfigurierten Agenten (nimmt jetzt Konfiguration entgegen)**
def erstelle_konfigurierten_agenten(config, workspace): # **Workspace als Parameter hinzugef√ºgt**
    """Erstellt einen KI-Agenten mit der gegebenen Konfiguration."""
    return KI_Agent(config, workspace) # Workspace weitergeben


# üöÄ Hauptprogramm ‚Äì Simulation des "Bewusstseins"
def simulation(config_path=None): # **Neu: config_path Parameter hinzugef√ºgt**
    workspace = GlobalWorkspace()

    # **Konfiguration laden**
    if config_path:
        config = lade_config_aus_yaml(config_path) # Konfiguration aus YAML laden
        print(f"‚öôÔ∏è Konfiguration geladen aus YAML-Datei: {config_path}")
    else:
        # **Default-Konfiguration (Dictionary-basiert), falls keine YAML-Datei angegeben**
        config = {
            'input_size': 3,
            'action_size': 2,
            'embed_dim': 32,
            'num_heads': 4,
            'state_similarity_threshold': 0.5,
            'emotion_dim': 3,
            'learning_rate': 0.01,
            'memory_capacity': 1000,
            'causal_capacity': 100,
            'prediction_model_hidden_size': 16,
            'layer_growth_threshold': 0.9,
            'novelty_threshold': 0.5,
            'novelty_tolerance': 0.5,
            'reward_history_window': 10,
            # **Beispiel-Callbacks (k√∂nnen in YAML oder Dictionary definiert werden, oder weggelassen werden)**
            'pre_decide_hook': pre_decide_callback_example, # Funktion als Callback
            'post_learn_hook': post_learn_callback_example, # Funktion als Callback
            'rule_engine_hook': rule_engine_callback_example # Funktion als Callback
        }
        print("‚öôÔ∏è Verwende Default-Konfiguration (Dictionary-basiert).")


    agent = erstelle_konfigurierten_agenten(config, workspace) # Agent mit Konfiguration erstellen, Workspace √ºbergeben

    # Stelle sicher, dass das gesamte Agentenmodell auf Float-Typ umgestellt ist.
    agent.float()

    rule_engine = workspace.get_rule_engine() # Rule Engine abrufen

    # **Demonstration: Direkter Zugriff auf Komponenten**
    print("\n‚ÑπÔ∏è Demonstration: Direkter Zugriff auf Agenten-Komponenten:")
    print(f"  - Ged√§chtnis-Kapazit√§t: {agent.memory_component.capacity}") # Zugriff auf Memory-Komponente
    print(f"  - Anzahl Attention-Ebenen: {agent.hierarchical_attention_component.num_layers}") # Zugriff auf Attention-Komponente
    print(f"  - Aktuelles Ziel-Key: {agent.self_model_component.current_goal_key}") # Zugriff auf Selbstmodell-Komponente

    for step in range(50):  # 500 Denkzyklen
        print(f"\nüîÑ Schritt {step}: {agent.current_goal}, Subziel: {agent.current_subgoal}") # Ziel- & Subziel-Objekt wird ausgegeben
        # 1. üåç Agent nimmt Welt wahr
        state = np.random.rand(agent.input_size)  # Sensorische Wahrnehmung (nutze konfigurierte input_size)

        # 2. üß† Agent denkt & entscheidet
        action = agent.decide(state)

        # 3. üîÑ Feedback erhalten
        reward = np.random.choice([1, -1])  # Positive/Negative Erfahrung
        next_state = state + np.random.normal(0, 0.1, agent.input_size) # Einfache Simulation des n√§chsten Zustands (nutze konfigurierte input_size)
        agent.learn(state, action, reward, next_state)

        # 4. üìù Agent speichert Gedanken
        memory = agent.memory.recall()
        if memory:
            workspace.share(f"Erinnerung: {memory}")

        # 5. üåê Workspace reflektiert √ºber Wissen
        reflection = workspace.access()
        if reflection:
            print(f"ü§î Der Agent reflektiert: {reflection}")

        # 6. ü§ñ Regelbasierter Innerer Dialog (rudiment√§r implementiert, zielbezogen)
        abstract_state_for_rules = agent.create_abstract_state() # Zustand einmal erstellen, um Ineffizienz zu reduzieren
        agent_state_for_rules = { # Beispiel f√ºr Agent-State, der an Rule-Engine √ºbergeben wird
            'novelty_bonus': agent.calculate_novelty_bonus(abstract_state_for_rules),
            'coherence_reward': agent.calculate_coherence_reward(agent.reflect(abstract_state_for_rules)),
            'frustration_drive': agent.self_model.drives["frustration"], # Drives aus Selbstmodell
            'complexity_metric': agent.hierarchical_attention.calculate_complexity(agent.hierarchical_attention(abstract_state_for_rules)), # Komplexit√§t berechnen
            'predictability': agent.world_model.calculate_state_properties(state, action).get('predictability', 0.0), # Predictability √ºbergeben (ACTION hinzugef√ºgt!)
            'valence': agent.world_model.calculate_state_properties(state, action).get('valence', 0.0), # Valence √ºbergeben (ACTION hinzugef√ºgt!)
            'current_goal_key': agent.current_goal_key, # Aktuellen Ziel-Key √ºbergeben (f√ºr robustere Regel-Engine)
            'current_subgoal': agent.current_subgoal, # Aktuelles Subziel √ºbergeben
            'emotion_state': agent.emotion_model.get_emotions().tolist(), # **Neu:** Emotionszustand √ºbergeben (als Liste f√ºr Rule-Engine)
            'self_awareness_vector': agent.self_model.self_awareness_vector.tolist() # **NEU**: Selbstbewusstseins-Vektor √ºbergeben
        }
        rule_engine.apply_rules(agent_state_for_rules, workspace, agent) # Rule-Engine anwenden, Agenten-Instanz √ºbergeben


# **Beispiel-Callback-Funktionen (k√∂nnen in der Konfiguration angegeben werden)**
def pre_decide_callback_example(agent_instance, state):
    """Beispiel f√ºr einen pre_decide_hook Callback."""
    print("\n[Callback - PRE-DECIDE]: Agent steht kurz vor der Entscheidung. Zustand:", state)
    # Hier k√∂nnte man z.B. den Zustand manipulieren oder zus√§tzliche Analysen durchf√ºhren, BEVOR die Entscheidung f√§llt.

def post_learn_callback_example(agent_instance, state, action, reward, next_state):
    """Beispiel f√ºr einen post_learn_hook Callback."""
    print("\n[Callback - POST-LEARN]: Agent hat gerade gelernt. Gelernte Erfahrung:", (state, action, reward, next_state))
    # Hier k√∂nnte man z.B. Daten loggen, Visualisierungen erstellen oder externe Systeme informieren, NACHDEM der Agent gelernt hat.

def rule_engine_callback_example(agent_instance, agent_state, phase):
    """Beispiel f√ºr einen rule_engine_hook Callback."""
    print(f"\n[Callback - RULE-ENGINE - {phase.upper()}]: Regel-Engine wird angewendet (Phase: {phase}). Agent-Zustand (Auszug):")
    print(f"  - Ziel: {agent_state.get('current_goal_key')}, Subziel: {agent_state.get('current_subgoal')}")
    print(f"  - Neuigkeitsbonus: {agent_state.get('novelty_bonus'):.2f}, Frustration: {agent_state.get('frustration_drive'):.2f}")
    # Hier k√∂nnte man z.B. eigene Regeln hinzuf√ºgen, die Rule-Engine erweitern oder die Ausgabe der Rule-Engine analysieren.


if __name__ == "__main__":
    # **1. Simulation mit Default-Konfiguration (Dictionary)**
    print("\n======================== Simulation 1: Default-Konfiguration (Dictionary) ========================")
    simulation()